# Chap.1 Introduction
Definition: An operating system is the layer of software that manages a computer’s resources for its users and their applications.

-Users
-User-mode: Apps, System Library?
-Kernel-mode: File System, TCP/IP networking, CPU scheduling, Virtual Memory
Hardware specific software, drivers
-Hardware: CPU, GPU, Network

- Resource Allocation
- Fault isolation
- Communication

Virtualization, virtual machine, guest operating system
Reliability, Availability (MTTF, MTTR)
Security, Privacy: Enforcement
Portable
AMI: API
Hardware Abstraction Layer (HAL)
Efficiency, Overload, Fairness
Response time, Throughput
Proprietary v.s. open system
Batch OS, DMA, multi-tasking
Time-sharing

Desktop, laptop, netbook: Win 7, MacOS, Linux
Smartphone OS: iOS, Android, Windows Phone, 
Embedded systems
Virtual machines: VMWare
Server OS, 

# Chap.2 Kernels and Processes
Source code -> executable image -> Process(machine instructions, data, heap, stack)
PCB (process control block)
Dual-mode: user-mode, kernel-mode (1-bit processor status register)
- previleged instructions
- Memory protection: base and bounds (unable to do 4 things: expandable heap and stack, memory sharing, non-relative memory address, memory fragmentation)
 (MS-DOS: no memory protection)
- Virtual address:
- Timer interrupts: os regain control from app, hardware timer (expires every 10 ms)
# Preemptive scheduling
# Three reasons why the kernel takes control from a user process: 
1. exceptions: divided by zero, write on read-only memory...
2. interrupts: external events occurred, I/O (mouse, Ethernet, WiFi, harddisk, keyboard)
# polling: kernel could loop to check each I/O device 
3. system calls: user transition to OS, request kernel do some ops on the user's behalf (use trap instruction)
# Kernel to user mode
new process, resume from exceptions/interrupts/system calls, switch to a different process, user-level upcall
Interrupt vector (a special register points to an area of kernel memory)
Interrupt stack
- If the process is running on the processor in user-mode, its kernel stack is empty, ready to be used for an interrupt.
- If the process is running on the processor in kernel-mode, e.g., due to an interrupt, exception or system call trap, its kernel stack is in use, containing the saved registers from the suspended user-level computation, as well as the current state of the kernel handler.
- If the process is available to run but is waiting for its turn on the processor, its kernel stack contains the registers and state to be restored when the process is resumed.
- If the process is waiting for an I/O event to complete, its kernel stack contains the suspended computation to be resumed when the I/O finishes.
Interrupt masking
- Multiple interrupts arrive asynchronously, defer and mask
# Context Switch
1. Save three key values (stack pointer, execution flags, instruction pointer)
2. Switch onto the kernel exception stack;
3. Push three key values onto the new stack;
4. Optionally save error code;
5. Invoke the interrupt handler;
# System Call
Pair of stubs: 
Kernel stub: 1. locate system call arguments; 2. validate parameters; 3. copy before check; 4. copy back any results;
# Upcalls: virtualized interrupts and exceptions
Linux: signals; Windows: asynchronous events;
Preemptive user-level thread package; Asynchronous I/O notification; Interprocess communication; User-level exception handling; User-level resource allocation policy;
# Architectural support for fast mode switches: SPARC
# Registers:
CS: code segment;
DS: data segment;
SS: stack segment;
ES: extra segment;
EAX: accumulator
EBX: base address
ECX: counter
EDX: to save reminder after division
ESI/EDI: source/destination index
EBP: base pointer, pointing to the bottom
ESP: stack (or heap?) pointer, pointing to the top
EIP:
EFLAGS:
# Case Study: booting an OS
1. Boot ROM: in x86, BIOS (Basic I/O System)
2. Bootloader (a bit for check), cryptographic signature, should be protected from virus
3. Load the OS from the file system;
4. Kernel starts first process: login
# Case Study: Virtual Machines
host OS v.s. guest OS

# Chap 3. The Programming Interface
Process Management, I/O, Thread Management, Memory management, 
File system and storage, networking and distributed system
Graphics and window management, Authentication and security
- Flexibility
- Safety
- Reliability: Improved reliability is another reason to keep the operating system kernel minimal
- Performance: transferring control into the kernel is more ex- pensive than a procedure call to a library,
# 3.1 Process Management
Shell: a job control system
parent/child process
- Windows process management: CreateProcess, 
- Unix process management: fork(), exec(program_name, Arguments[])
 fork() creates a complete copy of parent, then call exec()
# 5 steps of fork():
1. Create and initialize the process control block (PCB) in the kernel;
2. Create a new address space;
3. Initialize the address space with a copy of the entire contents of the address space of the parent;
4. Inherit the execution context of the parent (e.g., any open files)
5. Inform the scheduler that the new process is ready to run;
A strange aspect of UNIX fork is that the system call returns twice: once to the parent and once to the child. To the parent, UNIX returns the process ID of the child; to the child, it returns 0 indicating success
# 3 steps of exec()
1. Load the program prog into the current address space
2. Copy arguments args into memory in the address space
3. Initialize the hardware context to start execution at “start”
Note that exec does not create a new process!
As we discussed in the previous chapter, when a UNIX process finishes, it calls the system call exit(). PCB reclaimed when both parent and child have finished.
# wait() system call
pauses the parent until the child finishes, crashes, or is terminated
“UNIX wait” to refer to UNIX’s wait() system call
# 3.2 Input/Output
Basic Ideas:
1. Uniformity. All device I/O, file operations, and interprocess communication use the same set of system calls: open, close, read and write.
2. Open before use: open() returns a handle;
3. Kernel-buffered reads: Stream data, such as from the network or keyboard, is stored in a kernel buffer and returned to the application on request.
4. Kernel-buffered writes:
5. Explicit close: close()
For interprocess communication, we need two more system calls:
1. Pipes: (TCP is similar)
2. Replace file descriptor
3. Wait for multiple reads
### A list of system calls
# Create a child process as a clone of the current process; // fork returns to both the parent and child
fork ()

# Run the application ‘‘prog’’ in the current process
exec(prog , args)

# Tell the kernel the current process is complete,
# and its data structures should be garbage collected 
exit ()

# Pause until the child process has exited
wait(process_ID)

# Send an interrupt of ‘‘type’’ to a process
signal(process_ID , type)

# Open a file or hardware device , specified by ‘‘name ’’;
# returns a file descriptor that can be used by other calls
fd = open(name)

# Create a one-directional pipe between two processes;
# returns two file descriptors , one for reading , one for writing 
pipe(fd[2])

# Replace the to_fd file descriptor with a copy of from_fd; // used for replacing stdin/stdout
dup2(from_fd , to_fd)

# Read up to ‘‘size’’ bytes into buffer, from the device, file or channel.
# ‘‘read’’ returns the number of bytes actually read; for streaming
# devices this will often be less than ‘‘size’’.
# For example, a read from the keyboard device will return all of its queued bytes. 
int read(fd, buffer, size)

# Analogous to ‘‘read’’, write up to ‘‘size’’ bytes into kernel output // buffer for a device, file or channel.
# ‘‘write’’ normally returns immediately, but may stall if there is
# no space in the kernel buffer.
int write(fd, buffer, size)

# Return when any of the file descriptors in the array have data available to be read. // Returns the file descriptor with the data.
fd = select(fd[], number)

# Tell the kernel the process is done with this device, file, or channel.
close(fd)

# 3.3 Case Study: Implementing a Shell
stdin, stdout
- A program can be a file of commands.
- A program can send its output to a file.
- A program can read its output from a file.
- The output of one program can be the input to another program
# Producer/ Consumer

# 3.4 Case Study: Interprocess communication
Three widely used forms:
- Producer-consumer: Producer only writes, consumer only reads (Google's Mapreduce)
- Client-server: two-way, client sends requests, server replies back when complete
- File system: can be separated in time

# 3.5.1 Monolithic Kernels
Def: most of the operating system functionality runs inside the operating system kernel.
Hardware Abstraction Layer (HAL)
dynamically loadable device driver
# 3.5.2 Microkernel
Def: to run as much of the operating system as possible in one or more user-level servers.
Window manager
+ easier to modularize and debug user-level services
- little benefit to end users; slow down overall performance by inserting extra steps between the application and the services it needs.
In practice: hybrid model

# 3.6 Conclusion and Future Directions
Unix Interface (powerful and compact)
A key aspect of the UNIX interface are that creating a process (with fork) is separate from starting to run a program in that process (with exec);
another key feature is the use of kernel buffers to decouple reading and writing data through the kernel.

# Chap.4 Concurrency and Threads
Threads: A core abstraction for concurrency.
# 4.1 Threads: Abstraction and interface
Multi-threaded programs
- Program structure: Expressing logically concurrent tasks.
- Performance: Exploiting multiple processors.
- Performance: Coping with high-latency I/O devices. 
# Definition
A single execution sequence that represents a separately schedulable task.
Running, suspending, and resuming threads.
each thread runs on a dedicated virtual processor with unpredictable and variable speed
Cooperative multithreading v.s. Preemptive
# 4.2 Simple API and example
void sthread create(thread, func, arg)
void sthread yield()
int sthread join(thread)
void sthread exit(ret)
# 4.3 Thread internals
Thread control block (TCB) and per-thread state
- The state of the computation being performed by the thread
stack, Copy of processor registers
- Metadata about the thread that is used to manage the thread
Life Cycle:
Init -> Ready -> Running -> Waiting
# 4.4 Implementation details
# Create:
Allocating per-thread state ->
Initializing per-thread state.
# Delete:
remove it from the ready list so that it will never run again;
free the per-thread state we allocated for it.
# Thread context switch
PCB v.s. TCB: similar, all store registers when not running, address
process switch v.s. thread switch: Hardware-triggered (interrupts and exceptions.); Software-triggered (library calls v. system calls);
# Reasons for multi-threads
1. Program structure.
2. Exploiting multiple processors.
3. Coping with high latency I/O devices.
Similar mechanism like timer interrupt to suspend long-running thread
# Hybrid implementations. 
Although today few threads packages operate completely at user level, many split their work between the user-level library and the operating system kernel to reduce overheads by avoiding mode switches into the kernel for some of their operations.
# 4.5 Asynchronous I/O and event-driven programming
The basic idea is to allow a process to make a system call to issue and I/O request but not wait for the result.
read() v.s. aio_read()
Definition: event-driven programming pattern
# Comparison
1. Performance: Coping with high-latency I/O devices. Either approach— event-driven or threads—can be used to overlap I/O and processing.
Today, less clear-cut.
2. Performance: Exploiting multiple processors. The event-driven approach does not help a program exploit multiple processors.
3. Program structure: Expressing logically concurrent tasks.
# 4.6 Conclusion and future directions
Besides threads and event-driven programming, other approaches also parallel:
- Data parallel programs. Data parallel programming or SIMD (single instruction multiple data)
e.g., matrix manuplication, SQL, multimedia streams, GPU
- Distributed and parallel processing.

# Chap.5 Synchronizing Access to Shared Objects
Definition: independent threads
Definition: Cooperating threads: per-thread state (stack and registers), shared state (shared data on heap)
# 3 reasons of break down:
1. Program execution depends on the interleavings of threads’ access to shared state.
2. Program execution can be nondeterministic.
3. Compilers and architectures reorder instructions.
# Structured synchronization.
# 5.1 Challenges
- Race condition: when the behavior of a program depends on the interleaving of operations of di↵erent threads
- Atomic operations (ops can't be interleaved or split): a load or store of a 32-bit
- Too much milk problem
Safety, liveness
stable property
- Lock: ensures both, same codes on each 
# 5.2 Shared objects and synchronization
Synchronization variable
State variables
Atomic read-modify-write instructions
# 5.3 Lock: Mutual Exclusion
Lock::acquire(), Lock::release()
Formal properties
1. Mutual Exclusion: At most one thread holds the lock.
2. Progress.
3. Bounded waiting: If thread T attempts to acquire a lock, then there exists a bound on the number times other threads successfully acquire the lock before T does.

Critical sections: A sequence of code that operates on shared state

# 5.4 Condition Variables: Waiting for a Change
poll
CV::wait(Lock *lock)
CV::signal()
CV::broadcast()
Condition variables integrate with locks.
1. A condition variable is memoryless; the condition variable, itself, has no internal state other than a queue of waiting threads.
2. Wait() atomically releases the lock.
3. When a waiting thread is reenabled via signal() or broadcast(), it may not run immediately.

# 5.4.3 Example: Blocking bounded queue
Case study: Linux 2.6 kernel mutex lock
struct mutex {
/* 1: unlocked, 0: locked, negative: locked, possible waiters */
atomic_t count; 
pinlock_t ait_lock;
struct list_head wait_list; }; */

Acquiring the lock: thread called void mutex_lock(), defined in kernel/mutex.c

# Semaphore:
- A semaphore has a non-negative value
- When a semaphore is created, value can be set to any non-negative integer
- Semaphore::P() waits until value is positive. When value is positive, it atomically decrements value by 1.
- Semaphore::V() increments the value by 1, and if any threads are waiting in P(), one such thread is atomically enabled with its call to P() succeeding in decre- menting the value and returning.
Semaphore considered # harmful
# Once exception
There is one situation in which semaphores are often superior to condition variables and locks: synchronizing communication between an I/O device and threads running on the processor.

# 5.5 Implementing synchronization objects
# 5.5.1 Implementing uniprocessor locks by disabling interrupts
Multiprocessor spinlocks
Spinlocks are frequently used in multiprocessor kernels for shared objects whose methods all run fast.

# 5.6 Designing and implementing shared objects
• Decompose the problem into objects
• For each object
 – Define a clean interface
 – Identify the right internal state and invariants to support that interface
 – Implement methods with appropriate algorithms to manipulate that state.
• Add a lock
• Add code to acquire and release the lock 
• Identify and add condition variables
• Add loops to wait using the condition variable(s)
• Add signal()and broadcast()calls

Five Rules:
1. Always synchronize with locks and condition variables.
2. Always acquire the lock at the beginning of a method and release it right before the return.
3. Always hold the lock when operating on a condition variable.
4. Always wait in a while() loop.
5. (Almost) never sleep().

# Chap.6 Advanced Synchronization
# 6.1 Multi-object synchronization
Two-Phase Locking: expanding phase, contracting phase
Staged architectures
The key property of a staged architecture is that the state of each stage is private to that stage.
modularity benefits
improved cache locality
The special case when there is exactly one thread per stage is called event processing
Overload: The throughput of the system will be limited by that of the slowest stage.
# 6.2 Deadlock
Deadlock is a cycle of waiting among a set of threads where each thread is waiting for some other thread in the cycle to take some action.
mutually recursive locking
nested waiting

Deadlock v. starvation: both liveness concerns
In starvation, some thread fails to make progress for an indefinite period of time.

# Necessary Conditions for deadlock
1. Bounded resources.
2. No preemption.
3. Wait while holding.
4. Circular waiting.

Safe state:
Unsafe state:
Deadlocked state:

# Breaking deadlocks:
generally requires forcibly taking resources away from some or all of the deadlocked threads
Transactions:
1. choose one or more victem threads, stop them, undo actions, let others proceed;
2. restart after other threads proceed.
One key di↵erence between transactions and critical sections is that trans- actions can abort and roll back their actions.

# Detecting deadlock
# 6.3 Alternative approaches to synchronization
RCU (Read-Copy-Update)
lock-free and wait-free data structures
# RCU optimizes the read path to have extremely low synchroniza- tion costs, even if there are many concurrent readers.
1. Relax R/W semantics, grade period (update occurs until old version has been freed)
2. Restricted update rules
# Lock-free data structures.
When such updates are detected, the operation waits a finite amount of time for them to finish. If the conflicting operations do not finish, the operation either aborts the conflicting ones (rolling the object’s state back to what it was before that conflicting operations began) or it assists them (finishing the updates needed to complete that operation).
# 6.4 Conclusion
Resist the temptation to do complicated fine grained locking (let alone RCU)

# Chap.7 Scheduling 
# 7.1 Uniprocessor scheduling
workload
# compute bound
# I/O bound
# work-conserving: A scheduler is work-conserving if it never leaves the processor idle if there is work to do.
# 7.1.1 FIFO
average response time
Memcache
# 7.1.2 Shortest Job First (SJF)
art optimal
starvation
# 7.1.3 Round Robin
time quantum
Max-min fairness: Max- min fairness iteratively maximizes the minimum allocation given to a particular process
# 7.1.5 Case Study: Multi-level Feedback
- Responsiveness. Run short tasks quickly, as in SJF.
- Low Overhead. Minimize the number of preemptions, as in FIFO, and
minimize the time spent making scheduling decisions.
- Starvation-Freedom. All tasks should make progress, as in Round Robin.
- Background Tasks. Defer system maintenance tasks, such as disk de- fragmentation, so they do not interfere with user work.
- Fairness. Assign (non-background) processes approximately their max- min fair share of the processor.
# Multi-level Feedback Queue (MFQ)
An extension of Round Robin.
Instead of only a single queue, MFQ has multiple Round Robin queues, each with a different priority level and time quantum. Tasks at a higher priority level preempt lower priority tasks, while tasks at the same level are scheduled in Round Robin fashion. Further, higher priority levels have shorter time quanta than lower levels.

# Summary
• FIFO minimizes overhead.
• If tasks are variable in size, then FIFO can have very poor average response time.
• FIFO is optimal in terms of average response time if tasks are equal size.
• Consideringonlytheprocessor,SJFisoptimalintermsofaverageresponse
time.
• SJF is worst in terms of variance in response time.
• Round Robin approximates SJF if tasks have di↵erent sizes.
• Round Robin is the worst policy possible if tasks are of equal size.
• Tasks that intermix processor and I/O benefit from SJF and can do poorly under Round Robin.
• Max-min fairness can improve response time for I/O-bound tasks.
• By manipulating the assignment of tasks to priority queues, an MFQ
scheduler can achieve responsiveness, low overhead, and fairness

# 7.2 Multiprocessor scheduling
# 7.2.1 Scheduling Sequential Applications on Multiprocessors
Consider a server handling a very large number of web requests.
MFQ lock
# 7.2.2 Scheduling Parallel Applications
Oblivious Scheduling
- Bulk synchronous delay: Google MapReduce is a widely used bulk synchronous application.
- Producer-consumer delay.
- Critical path delay
- Preemption of lock holder
- I/O
# Co-scheduling
One possible approach to some of these issues is to schedule all of the tasks of a program together. This is called co-scheduling.
# Scheduler activations
# 7.3 Energy-aware scheduling
# 7.4 Real-time scheduling
Advanced scheduling with virtual time
BorrowedVirtualTime(BVT)
BVT runs process with lowest effective virtual time
# Case Study: SMART
Key idea: Separate importance from urgency
- Figure out which processes are important enough to run
- Run whichever of these is most urgent
# Importance=⟨priority,BVFT⟩ value tuple
- priority–parameter set by user or administrator (higher is better)
 Takes absolute priority over BVFT
- BVFT–Biased Virtual Finishing Time (lower is better)
 virtual time consumed + virtual length of next CPU burst

# Chap 8 Address Translation
- Protection. Keep each process isolated
- Sharing. Allow memory to be shared between processes (code segments, but also shared regions of memory, copy on write).
- Virtualization. Provide OS services transparently to applications: the illusion of infinite memory

# 8.1 Address translation concept
# 8.2 Segmentation and Paging
# 8.2.1 Segmentation
# 8.2.2 Paging
• Divide memory up into small pages
• Map virtual pages to physical pages
• Allow OS to gain control on certain operations
Trade-offs
• Eliminates external fragmentation
• Simplifies allocation, free, and backing storage (swap)
• Average internal fragmentation of .5 pages per "segment"

# 8.2.3 Multi-level paging
# 8.2.4 Multilevel segmentation and paging
# 8.3 Efficient address translation
# 8.3.1 Translation lookaside buffers
Software managed TLBs
On eachmemory reference
- Check TLB, if entry present get physic aladdress fast
- If not, walk page tables, insert in TLB for next time (Must evict some entry)
• TLB operates at CPU pipeline speed -> small,fast
• More Complex on a multi processor (TLB shootdown)

# 8.3.2 Virtually addressed caches
# 8.3.3 Consistency management
I/O, core map, TLB and page shootdown tagged TLBs and virtual caches
# 8.4 Software address translation 8.4.1 Software fault isolation
e.g., native client
# 8.4.2 Dynamic translation
Java and android virtual machines
# 8.5 Conclusions and future directions

# Chap 9 Caching and Virtual Memory
• When is caching useful? (e.g., if cost of regenerating the data is more than the cost of storing it until it is next reused)
• How do we know if cache is valid?
• How do we choose what to keep in the cache?
# 9.1 Cache concept: when it works and when it doesn’t
# 9.2 Hardware cache management

# 9.3 Memory mapped files and virtual memory
Advantages:
• Can re-locate program while running
• Most of a process’s memory may be idle (80/20 rule).
Challenge:
VM=extra layer, could be slow
# Idea 1: load-time linking
• Idea: link when process executed, not at compile time

# Idea 2: base + bound register
Two special privileged registers: base and bound
- Physical address = virtual address + base
- Check 0 <= virtual address < bound, else trap to kernel

• Advantages
- Cheap in terms of hardware: only two registers
- Cheap in terms of cycles: do add and compare in parallel
- Examples: Cray-1 used this scheme
• Disadvantages
- Growing a process is expensive or impossible
- No way to share code or data (E.g.,two copies of bochs, both running pintos)
• One solution: Multiple segments

• Advantages
- Multiple segments per process
- Allows sharing!(how?)
- Don't need entire process in memory
 Disadvantages
- Requires translation hardware, which could limit performance
- Segments not completely transparent to program (e.g., default segment faster or uses shorter instruction)
- n byte segment needs n contiguous bytes of physical memory
- Makes fragmentation a real problem.

# MMU
• Programs load/store to virtual addresses
• Actual memory uses physical addresses
• VM Hardware is Memory Management Unit(MMU)
Usually part of CPU

# Very different MMU: MIPS
• Hardware checks TLB on application load/store
  - References to addresses not in TLB trap to kernel
• Each TLB entry has the following fields:
  Virtual page, Pid, Page frame, NC, D, V, Global
• Kernel itself unpaged
  - All of physical memory contiguously mapped in high VM
   (hardwired in CPU, not just by convention as with Pintos)
  - Kernel uses these pseudo-physical addresses
• User TLB fault hander very efficient
  - Two hardware registers reserved for it
  - utlb miss handler can itself fault—allow paged page tables
• OS is free to choose page table format!

# 9.3.1 Memory allocation policies
FIFO, LRU, LFU, optimal, self-paging

# 9.3.2 Implementing memory allocation
- Proven: impossible to construct an "always good" allocator
• Problem:freecreatesholes(“fragmentation”)
  Result? Lots of free space but cannot satisfy request!

• Two factors required for fragmentation
 1. Different lifetimes— if adjacent objects die at different times, then fragmentation:
 2. Different sizes: If all requests the same size, then no fragmentation (that's why no external fragmentation with paging)

# Theoretical result:
- For any possible allocation algorithm, there exist streams of allocation and deallocation requests that defeat the allocator and force it into severe fragmentation.

# Best Fit:
minimize fragmentation by allocating space from block that leaves smallest fragment
# First Fit:
- Data structure: free list, sorted LIFO, FIFO, or by address

# Known patterns of real programs
Most real programs exhibit 1 or 2 (or all 3) of the following patterns of alloc/dealloc:
- Ramps: accumulate data monotonically over time
- Peaks: allocate many objects, use briefly, then free all
- Plateaus: allocate many objects, use for a long time

# Linking
How is a program executed?
• On Unix systems, read by "loader"
- Reads all code/data segments into buffer cache;
Maps code (read only) and initialized data (r/w) into addr space
- Or ... fakes process state to look like paged out

Lots of optimizations happen in practice:
 - Zero-initialized data does not need to be read in.
 - Demand load: wait until code used before get from disk
 - Copies of same program running? Share code
 - Multiple programs use same routines: share code

# Who builds what?
• Heap:allocated and laidout at runtime by malloc
 - Namespace constructed dynamically, managed by programmer
   (names stored in pointers, and organized using data structures)
 - Compiler, linker not involved other than saying where it can start
• Stack: allocated at runtime (func, .calls), layout by compiler
 - Names are relative off stack (or frame) pointer
 - Managed by compiler (alloc on procedure entry, free on exit)
 - Linker not involved because namespace entirely local:
 Compiler has enough information to build it.
• Global data/code: allocated by compiler, layout by linker
 - Compiler emits them and names with symbolic references
 - Linker lays them out and translates references
• Mmapped regions: Managed by programmer or linker
 - Some programs directly call mmap; dynamic linker uses it,too

# What is a library?
• A static library is just a collection of .o files
• When linking a .a (archive) file, linker only pulls in needed files
# Name mangling
• C++ can have many functions with the same name
• Compiler therefore mangles symbols
 - Makes a unique name for each function
 - Also used for methods/namespaces(obj::fn), template
   instantiations, & special functions such as operator new
# Initialization and destruction
• Initializers run before main
Throwing exceptions destroys automatic variables
# Dynamic (runtime) linking

# Static shared libraries
• Observation: everyone links in standard libraries (libc.a.),
 these libs consume space in every executable.
# Dynamic shared libraries

# Linking Summary
• Compiler/Assembler: 1 object file for each source file
• Linker: combines all object files into 1 executable file
• OS loader reads object files into memory
# Code = data, data = code

# Chap. 11 File Systems: Introduction and Overview
• Reliability.
• Large capacity and low cost.
• High performance.
• Named data.
• Controlled sharing.
Nonvolatile storage and file systems
nonvolatile storage; nonvolatile storage; stable storage
# 11.1 The file system abstraction
File. A file is a named collection of data in a file system.
File metadata: information about the file that is understood and managed by the operating system
File data
Directory: path, root directory, absolute path, relative path, current working directory, home directory
#!/bin/sh
#!/usr/bin/perl
Hard link: The mapping between a name and the underlying file
Volume: a collection of physical storage resources that form a logical storage device
Mount
# 11.2 API
Creating and deleting files.
Open and close
File access
# 11.3 Software layers
# 11.3.1 API and performance
Prefetching. Operating systems use prefetching to improve I/O performance.
# 11.3.2 Device drivers: Common abstractions
block device: allows data to be read or written in fixed-sized blocks (e.g., 512, 2048, or 4096 bytes)
DMA: direct memory access
# Interrupts. polling
# 11.3.4 Putting it all together: A simple disk request
When a process issues a system call like read() to read data from disk into the process's memory, the operating system moves the calling thread to a wait queue. Then, the operating system uses memory mapped I/O both to tell the disk to read the requested data and to set up DMA so that the disk can place that data in the kernel’s memory. The disk then reads the data and DMAs it into main memory; once that is done, the disk triggers an interrupt. The operating system's interrupt handler then copies the data from the kernel's buffer into the process's address space. Finally, the operating system moves the thread the ready list. When the thread next runs, it will returns from the system call with the data now present in the specified buffer.

# Chap. 12 Storage Devices
# 12.1 Magnetic disk
platters; surfaces; spindle; head; head crash; arm assembly; arm;
sectors (fixed size, typically 512 bytes); track: a circle of sectors on a surface;
track skewing; sector sparing; slip sparing; buffer memory; track buffer; write acceleration; tag command queuing; native command queuing
Serial ATA (SATA)
# 12.1.1 Disk access and performance
LBAs: logical block addresses
disk access time = seek time + rotation time + transfer time (track + sector + read/write to/from buffer)
# 12.1.2 Disk scheduling
FIFO, or FCFS (First Come First Served)
+ Easy to implement
+ Good fairness
- Cannot exploit request locality
- Increases average latency, decreasing throughput
SPTF/SSTF. (shortest positioning time first/shortest seek time first)
+ Exploits locality of disk requests
+ Higher throughput
- Starvation
- Don't always know what request will be fastest
Elevator-based algorithm: SCAN, and CSCAN.
 Sweep across disk, servicing all requests passed
 CSCAN: Only sweep in one direction
   Very commonly used algorithm in Unix
+ Takes advantage of locality 
+ Bounded waiting
- Cylinders in the middle get better service
- Might miss locality SPTF could exploit
# 12.2 Flash storage
solid state storage: 
+ no moving parts, flash storage can have much better random IO performance than disk and it can use less power and be less vulnerable to physical damage;
- more expensive
NOR, NAND (dense)
Flash storage access and performance, 3 operations:
1. Erase erasure block;
2. Write page;
3. Read page;
Durability; wear out; read disturb error;

# Chap. 13 Files and Directories
Trends
• Disk bandwidth and cost/bit improving exponentially
• Seek time and rotational delay improving very slowly

- Structure tracking a file's sectors is called an index node or inode
# 13.1 Accessing files: API and caching
# 13.2 Files: Placing and finding data
# 13.2.1 FAT: Linked list
# 13.2.2 FFS: Fixed tree
# 13.2.3 NTFS: Flexible tree with extents
# 13.2.4 Write anywhere file systems
# 13.3 Directories: Naming data
# 13.4 Putting it all together: File access in FFS
# 13.5 Alternatives to file systems

# Chap. 14 Network
7 Layers: Physical, Data link, Network, Transport, Session, Presentation, Application
4 Layer: Network Access (Ethernet), IP, TCP (host-to-host), Applications
• UDP and TCP most popular protocols on IP
• UDP – unreliable datagram protocol
• TCP – transmission control protocol
